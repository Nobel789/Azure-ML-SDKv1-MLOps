{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step process for monitoring deployed models\n",
        "\n",
        "Follow these steps to set up comprehensive monitoring for your deployed machine learning model. Each step will guide you through essential practices to ensure that your model remains accurate, reliable, and responsive in a production environment.\n",
        "\n",
        "    Step 1: Choose the right monitoring tools.\n",
        "\n",
        "    Step 2: Implement data quality checks.\n",
        "\n",
        "    Step 3: Set up performance metrics monitoring.\n",
        "\n",
        "    Step 4: Configure alerts for critical thresholds.\n",
        "\n",
        "    Step 5: Detect model drift.\n",
        "\n",
        "    Step 6: Log and troubleshoot."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Choose the right monitoring tools\n",
        "\n",
        "Select tools based on your deployment environment and monitoring needs:\n",
        "\n",
        "    Azure Monitor: provides comprehensive monitoring for models deployed in Azure environments, including integration and visualization.\n",
        "\n",
        "    Grafana: an open-source tool for creating real-time dashboards to visualize metrics.\n",
        "\n",
        "    Prometheus: suitable for monitoring  collecting and storing metrics at both the model and system level."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Implement data quality checks\n",
        "\n",
        "Run the following code in a Jupyter Notebook. Make sure to use the Python 3.8 - AzureML kernel and monitor incoming data quality to ensure consistency with training data:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample incoming data\n",
        "incoming_data = pd.DataFrame({'feature1': [1.4, 1.6, 1.8], 'feature2': [3.3, 3.9, 4.2]})\n",
        "\n",
        "# Training data metrics\n",
        "training_mean = {'feature1': 1.5, 'feature2': 3.7}\n",
        "training_std = {'feature1': 0.2, 'feature2': 0.3}\n",
        "\n",
        "# Calculate statistics for incoming data\n",
        "incoming_mean = incoming_data.mean()\n",
        "\n",
        "# Compare data to check for drift\n",
        "for feature in incoming_data.columns:\n",
        "    if abs(incoming_mean[feature] - training_mean[feature]) > training_std[feature] * 3:\n",
        "     print(f\"Alert: Significant data deviation detected in {feature}\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1763985913190
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Set up performance metrics monitoring with prometheus\n",
        "\n",
        "One way to monitor performance metrics is by using Prometheus, an open-source cloud monitoring toolkit. \n",
        "\n",
        "Continuously track metrics such as accuracy, precision, recall, and F1 score:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Prometheus configuration to scrape model metrics\n",
        "scrape_configs:\n",
        "  - job_name: 'model_metrics'\n",
        "    static_configs:\n",
        "      - targets: ['localhost:9090']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Configure alerts for critical thresholds\n",
        "\n",
        "You can also use Azure to configure alerts and monitoring. Read and understand (but donâ€™t run) this example code:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.monitor.query import MetricsQueryClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "client = MetricsQueryClient(credential)\n",
        "\n",
        "# Alert example\n",
        "alert_condition = {\n",
        "    \"threshold\": 85,\n",
        "    \"metric\": \"accuracy\",\n",
        "    \"operator\": \"LessThan\",\n",
        "    \"alert_action\": \"EmailNotification\"\n",
        "}\n",
        "\n",
        "print(\"Alert condition set for accuracy below 85%.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Detect model drift\n",
        "\n",
        "Use statistical tests to monitor for model drift caused by changes in data distributions:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Training data and incoming data samples\n",
        "training_data_sample = np.random.normal(1.5, 0.2, 100)\n",
        "incoming_data_sample = incoming_data['feature1']\n",
        "\n",
        "# Perform KS test\n",
        "d_stat, p_value = ks_2samp(training_data_sample, incoming_data_sample)\n",
        "if p_value < 0.05:\n",
        "    print(\"Model drift detected for feature1\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1763986372918
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Log and troubleshoot\n",
        "\n",
        "Log inputs, outputs, and errors to diagnose issues in production:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging settings\n",
        "logging.basicConfig(filename='model_performance_logs.log', level=logging.INFO)\n",
        "\n",
        "# Log prediction request and response\n",
        "input_data = {'feature1': 1.6, 'feature2': 3.8}\n",
        "output_prediction = {'prediction': 0.9}\n",
        "\n",
        "logging.info(f\"Input: {input_data}, Output: {output_prediction}\")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1763986457193
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "In this activity, you learned how to:\n",
        "\n",
        "    Choose the right tools for monitoring deployed models.\n",
        "\n",
        "    Implement data quality checks to ensure consistency.\n",
        "\n",
        "    Track performance metrics continuously.\n",
        "\n",
        "    Configure alerts to address critical thresholds.\n",
        "\n",
        "    Detect model drift effectively.\n",
        "\n",
        "    Utilize logging for troubleshooting and diagnostics.\n",
        "\n",
        "Effective monitoring ensures that your models perform reliably in dynamic environments. By automating monitoring tasks and using scalable tools, you can maintain model performance and deliver consistent results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}