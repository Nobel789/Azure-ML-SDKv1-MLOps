{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ART 1 — Setting Up Monitoring With Azure Metrics\n",
        "Code"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.monitor.query import MetricsQueryClient\n",
        "\n",
        "# Connect to Azure Metrics Client\n",
        "credential = DefaultAzureCredential()\n",
        "client = MetricsQueryClient(credential)\n",
        "\n",
        "# Define alert conditions\n",
        "alert_conditions = {\n",
        "    \"metric_name\": \"response_time\",\n",
        "    \"threshold\": 200,\n",
        "    \"operator\": \"GreaterThan\",\n",
        "    \"alert_action\": \"EmailNotification\"\n",
        "}\n",
        "print(\"Alert set up for response time exceeding 200 ms.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Alert set up for response time exceeding 200 ms.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1764433646799
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2 — Simulating Response Time Alerts"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "# Simulate response time metric\n",
        "response_time = 200  # Normal response time in milliseconds\n",
        "threshold = 300  # Alert threshold in milliseconds\n",
        "\n",
        "# Simulate an increase in response time\n",
        "response_time += random.randint(100, 200)  # Add random delay to exceed the threshold\n",
        "\n",
        "# Check if the response time exceeds the threshold\n",
        "if response_time > threshold:\n",
        "    print(f\"Alert: Response time exceeded! Current response time: {response_time} ms\")\n",
        "    # Trigger notification\n",
        "    print(\"Notification sent: Response time alert.\")\n",
        "    # Placeholder for initiating remediation (e.g., scaling up resources)\n",
        "    print(\"Initiating remediation: Scaling up resources.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Alert: Response time exceeded! Current response time: 321 ms\nNotification sent: Response time alert.\nInitiating remediation: Scaling up resources.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1764433691519
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 3 — Monitoring Model Accuracy"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "# Simulate model accuracy metric\n",
        "model_accuracy = 0.85  # Normal accuracy\n",
        "threshold_accuracy = 0.80  # Minimum acceptable accuracy\n",
        "\n",
        "# Simulate a drop in accuracy\n",
        "model_accuracy -= random.uniform(0.1, 0.15)  # Decrease accuracy below the threshold\n",
        "\n",
        "# Check if the model accuracy drops below the threshold\n",
        "if model_accuracy < threshold_accuracy:\n",
        "    print(f\"Alert: Model accuracy dropped! Current accuracy: {model_accuracy:.2f}\")\n",
        "    # Trigger notification\n",
        "    print(\"Notification sent: Model accuracy alert.\")\n",
        "    # Placeholder for initiating remediation (e.g., retraining the model)\n",
        "    print(\"Initiating remediation: Retraining the model.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Alert: Model accuracy dropped! Current accuracy: 0.74\nNotification sent: Model accuracy alert.\nInitiating remediation: Retraining the model.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1764433729702
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Print actions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Notification sent: Model accuracy alert.\")\n",
        "print(\"Initiating remediation: Retraining the model.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Notification sent: Model accuracy alert.\nInitiating remediation: Retraining the model.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1764433757079
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}